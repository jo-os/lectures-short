# Базы данных, их типы

**Базы данных (БД)** — это структурная совокупность взаимосвязанных данных определённой предметной области: реальных объектов, процессов, явлений и т. д.

**База данных** — совокупность данных, организованных в соответствии с концептуальной структурой, описывающей характеристики этих данных и взаимоотношения между ними, которая поддерживает одну или более областей применения

Существует множество определений термина «База Данных» и из всех этих определений можно выделить ключевые моменты:
- БД хранится и обрабатывается в вычислительной системе;
- Данные в БД логически структурированы;
- БД включает схему или метаданные, описывающие её структуру.

Первый момент является строгим, а другие допускают различные трактовки и различные степени оценки

Основная функция базы данных — предоставление единого хранилища для всей информации, относящейся к определённой теме или предметной области. База данных может содержать всё, что угодно: будь то список приглашённых на свадьбу гостей или информация о каждом клиенте, посетившем web-сайт электронного магазина и разместившего там свои заказы.

**Архитектурные модели** - разделяют на три основных вида, появление которых напрямую связано с развитием технологий и программного обеспечения:
- **Централизованная** - Система состоит из центрального компьютера и подключенных к нему терминалов. Вся обработка данных выполняется в рамках этого единственного компьютера. Пользовательские терминалы не обладают вычислительными мощностями и являются простым интерфейсом. Центральный компьютер производит все вычислительные действия и обработку данных, передавая результат на терминалы.
- **Файл — сервер** - Система состоит из файлового сервера, который содержит файлы, необходимые для работы приложений и самой СУБД. Пользовательские приложения и полные копии СУБД размещены и функционируют на отдельных рабочих станциях и обращаются к файловому серверу только по мере необходимости получения доступа к нужным им файлам. Фактически, файловый сервер используется просто как общий жесткий диск. Поскольку файловый сервер не воспринимает команд на языке SQL, то СУБД должна запросить у файлового сервера файлы, необходимые для совершения запроса, что усложняет управление параллельной работой и контроль целостности данных, так как доступ к одним и тем же файлам могут осуществлять сразу несколько экземпляров СУБД
- **Клиент – сервер** - Данная архитектура предполагает, что СУБД находится на сервере, и только она имеет доступ к файлам БД. На клиентских компьютерах работают пользовательские приложения и клиентские компоненты СУБД, осуществляющие взаимодействие с сервером. От клиента на сервер приходят запросы, которые обрабатываются СУБД, и результат отправляется обратно клиенту. Таким образом повышается производительность системы, гарантируется согласованность и целостность данных. Об этой модели более подробно поговорим позже, на примере PostgreSQL

**СУБД** — это программа, с помощью которой осуществляется хранение, обработка, управление и поиск информации в базе данных.

СУБД используются для выполнения различных операций с данными:
- Ввод
- Хранение
- Манипулирование
- Обработка запросов к БД
- Выборка
- Сортировка
- Обновление
- Поиск
- Защита данных от несанкционированного доступа или потери

СУБД можно разделить на следующие категории:
- Реляционные
- Объектно-ориентированные
- NoSQL
  - Иерархические
  - Графовые
  - Сетевые
  - Документо-ориентированные
  - Ключ-значение
  - Column-oriented
- Протокол LDAP

**Реляционные СУБД** - В системах управления реляционными базами данных отношения между данными являются реляционными, и данные хранятся в виде таблиц. Каждый столбец таблицы представляет атрибут, а каждая строка в таблице представляет собой запись. Каждое поле в таблице представляет собой значение данных. Для взаимодействия с данными используют SQL 

**Объектно-ориентированные СУБД** - Предоставляют полнофункциональные возможности программирования БД, сохраняя при этом совместимость с ООП языком. Добавляет функциональность базы данных в ООП языки программирования

**NoSQL СУБД** - Базы данных NoSQL не используют SQL в качестве основного языка доступа к данным. NoSQL не имеет предопределенных схем, что делает её идеальным кандидатом для быстро меняющихся сред разработки. NoSQL позволяет разработчикам вносить изменения «на лету», не затрагивая приложения.
- **Иерархические СУБД** - В иерархической модели данные организованы в древовидную структуру. Данные хранятся в виде набора полей, где каждое поле содержит только одно значение. Записи связаны друг с другом через связи в отношениях родитель-потомок. В иерархической модели базы данных каждая дочерняя запись имеет только одного родителя. Родитель может иметь несколько детей.
- **Сетевые СУБД** - Сетевые СУБД используют сетевую структуру для создания отношений между объектами. Сетевые базы данных имеют иерархическую структуру, но в отличие от иерархических баз данных, где у одного дочернего элемента может быть только один родитель, сетевой узел может иметь отношения с несколькими объектами. Сетевая база данных больше похожа на «паутину»
- **Графовые СУБД** - NoSQL БД, которая использует структуру графов для семантических запросов. Данные хранятся в виде узлов, ребер и свойств. Узел представляет собой объект. Ребро представляет собой отношение, которое соединяет узлы. Свойства — это дополнительная информация, добавляемая к узлам.
- **Документо-ориентированные СУБД** - Является NoSQL БД, в которой данные хранятся в виде документов. Каждый документ представляет данные в виде ключ-значение, связь с другими документами и мета-полями.
- **Ключ-значение СУБД** - База данных на основе пар «ключ‑значение» хранит данные как совокупность пар «ключ‑значение», в которых ключ служит уникальным идентификатором. Как ключи, так и значения могут представлять собой что угодно: от простых до сложных составных объектов.
- **Column — oriented СУБД** - В таких системах данные хранятся в виде матрицы, строки и столбцы которой используются как ключи. Типичным применением этого типа СУБД является веб-индексирование, а также задачи, связанные с большими данными, с пониженными требованиями к согласованности. Каждая строка имеет свой набор столбцов
- **LDAP СУБД** - Lightweight Directory Access Protocol — это открытый и кроссплатформенный протокол, используемый для аутентификации служб каталогов. LDAP — относительно простой протокол, использующий TCP/IP и позволяющий производить операции аутентификации (bind), поиска (search) и сравнения (compare), а также операции добавления, изменения или удаления записей. Имеет преимущество в масштабируемых системах аутентификации перед РСУБД за счет каскадной репликации

**Реляционная модель**

Модель представляет собой фиксированную структуру математических понятий, которая описывает, как будут представлены данные. Базовой единицей данных в пределах реляционной модели является таблица. Таблица — это базовая единица данных. В реляционной алгебре она называется «отношение» (relation). Состоит из атрибутов (columns), которые определяют конкретные типы данных. Данные в таблице организованы в кортежи (rows), которые содержат множества значений столбцов.

Возвращаясь к архитектурным моделям, давайте на примере PostgreSQL разберем, из каких подсистем она состоит:
- Клиентская часть включает клиентское приложение и библиотеку LIBPQ, реализующую интерфейс связи с сервером. Библиотека LIBPQ отвечает за установление соединения с сервером и передачу SQL запросов
- Серверная часть включает серверные процессы и контролирующий процесс-демон Postmaster, отвечающий за взаимодействие с клиентами.

Postmaster авторизует и принимает запросы от клиентов и осуществляет обмен данными между клиентом и сервером. При получении запроса соединения от клиента Postmaster создаёт соответствующий фоновый серверный процесс postgres, при этом используется связь один-к-одному. После того как серверный процесс создан, клиент и сервер взаимодействуют напрямую.

Хранение и управление данными — несколько серверных процессов могут иметь одновременный доступ к информации из хранилища.

**Процессы и память**

После того как клиент соединяется с сервером, процесс postmaster создает серверный процесс, и дальше клиент работает уже с ним. На каждое соединение создается по серверному процессу, поэтому при большом числе соединений следует использовать пул (расширение pgbouncer). Postmaster также запускает ряд служебных процессов, увидеть которые можно с помощью команды ps fax. У экземпляра базы данных имеется общая для всех серверных процессов память. Большую часть памяти занимает буферный кэш (shared buffers), необходимый для ускорения работы с данными на диске. PostgreSQL полностью полагается на операционную систему и сам не управляет устройствами. Например, он считает, что вызов fsync() гарантирует попадание данных из памяти на диск.

Кроме буферного кэша в общей памяти находится информация о блокировках и многое другое, через нее же серверные процессы общаются друг с другом. У каждого серверного процесса есть своя локальная память. В ней находится:
- кэш каталога (часто используемая информация о базе данных),
- планы запросов,
- рабочее пространство для выполнения запросов,
- и другое.

**Буферный кэш** - Используется для оптимизации скорости работы памяти и дисков. Он состоит из массива буферов, которые содержат блоки данных и дополнительную информацию об этих блоках. Размер блока обычно составляет 8 КБ, но может устанавливаться при сборке. Буферный кэш, как и другие структуры в памяти, защищен блокировками от одновременного доступа. Блокировки организованы достаточно эффективно, чтобы не создавать большой конкуренции. Любой блок, с которым работает СУБД, попадает в кэш. Часто используемые блоки остаются в кэше надолго, редко используемые — вытесняются и заменяются другими блоками. Буфер, содержащий измененный блок, называется «грязным». Процесс Background Writer постепенно записывает такие блоки на диск в фоновом режиме, это позволяет снизить нагрузку на диски и увеличить производительность. Если Background Writer не успевает записать вытесняемый серверным процессом грязный буфер, то процесс записывает его сам. С точки зрения производительности этого лучше не допускать

**Кластеры** - Кластер баз данных представляет собой набор баз, управляемых одним экземпляром работающего сервера. После инициализации кластер будет содержать базу данных с именем postgres, предназначенную для использования по умолчанию утилитами, пользователями и сторонними приложениями. Сам сервер баз данных не требует наличия базы postgres, но многие внешние вспомогательные программы рассчитывают на её существование. Хранение данных на диске организовано с помощью табличных пространств. Табличное пространство указывает расположение данных (каталог на файловой системе). Оно может использоваться несколькими базами данных. Например, можно организовать одно табличное пространство на быстрых дисках для активно использующихся данных и другое — на медленных дисках для архивных данных. Объекты (таблицы и индексы) хранятся в файлах. Каждый объект занимает один или несколько файлов внутри каталога табличного пространства. Кроме того, файлы разбиваются на части по 1 ГБ. Необходимо учитывать влияние потенциально большого количества файлов на используемую файловую систему

**Транзакции** — это фундаментальное понятие во всех СУБД. Суть транзакции в том, что она объединяет последовательность действий в одну операцию «всё или ничего». Промежуточные состояния внутри последовательности не видны другим транзакциям, и если что-то помешает успешно завершить транзакцию, ни один из результатов этих действий не сохранится в базе данных. Транзакции должны удовлетворять требованиям ACID: атомарность (atomicity), согласованность (consistency), изоляция (isolation) и долговечность (durability)
- Атомарность означает, что при фиксации выполняются все операции, составляющие транзакцию, при откате — не выполняется ни одна.
- Согласованность — поддержание целостности данных. Транзакция начинает работу в согласованном (целостном) состоянии и по окончании своей работы также оставляет данные согласованными.
- Изоляция означает, что на результат работы транзакции не оказывают влияния другие, одновременно с ней выполняющиеся транзакции. По стандарту изоляция может иметь несколько различных уровней, в различной степени защищающих транзакции от внешних воздействий.
- Долговечность подразумевает возможность восстановить базу данных после сбоя в согласованном состоянии

**CAP**

**Теорема CAP (теорема Брюера)** — эвристическое утверждение о том, что в любой реализации распределенных вычислений возможно обеспечить не более двух из трёх следующих свойств:
- Согласованность данных (consistency) — как только мы успешно записали данные в наше распределенное хранилище, любой клиент при запросе получит эти последние данные.
- Доступность (availability) — в любой момент клиент может получить данные из нашего хранилища или получить ответ об их отсутствии, если их никто еще не сохранял.
- Устойчивость к разделению (partition tolerance) — потеря сообщений между компонентами системы (возможно даже потеря всех сообщений) не влияет на работоспособность системы. Здесь очень важный момент: если какие-то компоненты выходят из строя, то это тоже подпадает под этот случай, так как можно считать, что данные компоненты просто теряют связь со всей остальной системой.

**Классы систем**
- В системе класса **CA** во всех узлах данные согласованы, и обеспечена доступность, при этом она жертвует устойчивостью к распаду на секции.
- Система класса **CP** в каждый момент обеспечивает целостный результат и способна функционировать в условиях распада, но достигает этого в ущерб доступности: может не выдавать отклик на запрос.
- В системе класса **AP** не гарантируется целостность, но при этом выполнены условия доступности и устойчивости к распаду на секции. Хотя системы такого рода известны задолго до формулировки принципа CAP (например, распределённые веб-кэши или DNS).

Несмотря на то, что РСУБД относят к СА, сложно представить системы не устойчивые к разделению. Как правило, приходится выбирать между PA и PC, в зависимости от бизнес-задач

**PACELC**

Расширение CAP-теоремы. Добавляет понятие Latency — время, за которое клиент получит ответ и которое регулируется каким-либо уровнем согласованности. В случае разделения сети (P) в распределённой системе необходимо выбирать между доступностью (A) и согласованностью (C) (согласно теореме CAP), но в любом случае, даже если система работает нормально в отсутствии разделения (E), нужно выбирать между задержками (L) и согласованностью (C)

**NoSQL**

**BASE** — принцип, противопоставляющий себя ACID.
- BA — basically availability (базовая доступность) деградация части узлов ведет к деградации части сессий, исключая полную деградацию системы. Система отвечает на любой запрос, но в ответе могут быть неверные данные.
- S — soft state (неустойчивое состояние) уменьшение времени хранения сессий и фиксация обновлений только критичных операций.
- E — eventually consistent (конечная согласованность) изменение состояния в конечном итоге применится на все системы. BASE позволяет проектировать высокопроизводительные системы

**Преимущества NoSQL**
- Гибкость
- Масштабируемость
- Высокая производительность
- Широкие функциональные возможности

**NoSQL. MongoDB**
MongoDB — одна из популярных документо-ориентированных СУБД. MongoDB поддерживает:
- ad-hoc запросы,
- индексирование,
- горизонтальное масштабирование и шардинг,
- MapReduce,
- транзакции, ACID/BASE.
По PACELC теореме MongoDB соответствует PA/EC.

Модель устройства базы данных в MongoDB:
- База данных состоит из коллекций;
- Центральным понятием является документ;
- Документ представляет набор пар ключ-значение.

# Кеширование Redis/memcached

**Кэширование** — это способ оптимизации работы приложения, при котором данные временно кладутся в промежуточный буфер с быстрым доступом. Выбор буфера для кэша зависит от решаемой задачи и может быть как памятью сервера, так и файлом, базой данных или любой другой сущностью, куда можно положить данные. Обычно данные в кэше — это данные, к которым наиболее часто осуществляется запрос

**Важно учесть при работе с кэшом**
- Инвалидация. Если данные изменяемые (например, кэш ответов из базы данных), то их нужно будет инвалидировать при изменении в источнике;
- TTL (time-to-live). Кэш не может жить вечно, и нужно быть готовым к тому, что он «протухнет» или «потеряется в любой момент»;
- Кэшировать данные нужно только при необходимости, так как это усложняет архитектуру и может добавить проблем в эксплуатации.

**Какие проблемы призвано решить кэширование?**
- Повышение производительности достигается за счет складывания в кэш данных, к которым чаще всего происходит обращение;
- Увеличение скорости ответа;
- Экономия ресурсов базы данных, например, применяя кэширование тяжелых запросов;
- Сглаживание бустов трафика. Например, во время черной пятницы онлайн-магазины используют кэш, чтобы переживать резкое увеличение трафика.

Понятие кэширования очень растяжимое:
- DNS;
- CDN;
- HTTP-кэширование;
- Кэш процессора;
- Тяжелые запросы в базу данных.

**Алгоритмы кэширования**

Самая оптимальная стратегия кэширования — это подход, при котором мы кэшируем только то, что будем использовать в будущем и убираем из кэша то, что использовать не будем. Однако, чтобы добиться этого, нужно уметь предсказывать будущее.

**LRU (least recently used)** - LRU (вытеснение давно неиспользуемых) — это алгоритм, при котором вытесняются значения, которые дольше всего не запрашивались. Соответственно, необходимо хранить время последнего запроса к значению. И как только число закэшированных значений превосходит N, необходимо вытеснить из кеша значение, которое дольше всего не запрашивалось.

**PLRU (pseudo least recently used)** - PLRU (псевдо-LRU) алгоритм призван улучшить производительность LRU для кэша с большой ассоциативностью. При помощи двоичного дерева для поиска алгоритм эффективно понимает, где самый молодой элемент кэша, а где самый старый. Может быть неоптимальным и неверно вытеснять записи.

**LFU (Least-Frequently Used)** - LFU (наименее часто используемый) — из кэша вытесняются элементы, которые реже всего используются. Для этого у каждого элемента в кэше ведется счетчик обращения к нему. В таком виде алгоритм может давать сбои и вытеснять только что добавленные элементы, к которым еще ни разу не было повторного обращения. Редко можно встретить реальное использование.

**Memcached**

Бесплатная высокопроизводительная система кэширования с открытым исходным кодом. Универсальная по своей природе, но предназначенная для ускорения работы динамических веб- приложений за счет снижения нагрузки на базу данных. **Memcached** — это in-memory хранилище значений ключей для небольших фрагментов произвольных данных (строк, объектов) из результатов вызовов баз данных, API или рендеринга страниц.
- Был написан в LiveJournal Брэдом Фицпатриком и выпущен в OSS в 2003 году. С тех пор оброс большим комьюнити.
- Является простым и популярным решением для in-memory кэширования даже в наши дни.
- Из коробки умеет шардировать ключи с помощью клиентской библиотеки.
- Работает на порту 11211.
- Работает с кэшом по алгоритму LRU с TTL.
База сохраняет свою простоту, но:
- Нет неймспейсов. Чтобы отделить ключи одного приложения от другого, нужно либо использовать некрасивые префиксы, либо запускать отдельный инстанс memcached;
- Нельзя флашнуть сабсет ключей. Из-за того что ключи никак не сгруппированы, нельзя удалить часть и нельзя запросить часть для удаления;
- In-memory. Рестарт базы удалит все данные;
- Нет репликации из коробки;
- Нет типизации. Все данные — это строки.
**Установка memcached на сервер с Debian 10 с помощью apt:**
```
$ sudo apt update && apt install memcached
```
Убеждаемся, что база запустилась:
```
$ systemctl status memcached
● memcached.service - memcached daemon
Loaded: loaded (/lib/systemd/system/memcached.service; enabled; vendor preset: enabled)
Active: active (running) since Sun 2021-08-15 21:10:46 UTC; 1s ago
Docs: man:memcached(1)
Main PID: 1591 (memcached)
```
Если клиентом memcached будет не localhost, то переопределяем IP-
адрес, который слушает база в конфиге /etc/memcached.conf:
```
# Specify which IP address to listen on. The default is to listen on all IP
addresses
# This parameter is one of the only security measures that memcached has, so
make sure
# it's listening on a firewalled interface.
-l 127.0.0.1
```
Memcached сам по себе считается хранилищем с бесконечным горизонтальным масштабированием. Однако, оно не отказоустойчивое и для него в редких случаях требуется добавление репликации между нодами. Эту задачу решает mcrouter. Он выступает проксей между клиентом и нодами memcached, предоставляя возможности репликации данных.

**Redis**

**Redis (remote dictionary server)** — это in-memory хранилище структур данных с открытым исходным кодом, используемое в качестве базы данных, кэша и очереди сообщений.

Отличительные характеристики:
- Есть неймспейсы. На старте базы их 16, но это значение меняется в конфиге;
- In-memory. Но данные персистентно хранятся на диске;
- Репликация из коробки;
- Типизация. Есть поддержка разных типов данных;
- Lua-скриптинг. Позволяет писать свои хранимые процедуры;
- Транзакции.
Политики вытеснения работают только тогда, когда у redis закончилось место в памяти:
- noeviction: возвращает ошибки, когда достигнут предел памяти и клиент пытается выполнить команды, которые могут привести к увеличению объема используемой памяти;
- allkeys-lru: ключи удаляются по алгоритму LRU;
- volatile-lru: удаляются только ключи с истекшим TTL;
- allkeys-random: случайное удаление ключей;
- allkeys-random: случайное удаление ключей;
- volatile-random: случайное удаление ключей с истекшим TTL;
- volatile-ttl: удаление ключей с истекшим TTL и с наиболее коротким TTL;
- volatile-lfu: удаляются ключи с истекшим TTL по LFU;
- allkeys-lfu: все ключи удаляются по LFU
**Установка redis на сервер с Debian 10 с помощью apt:**
```
$ sudo apt update && apt install redis
```
Убеждаемся, что база запустилась:
```
$ systemctl status redis
● redis-server.service - Advanced key-value store
Loaded: loaded (/lib/systemd/system/redis-server.service; enabled; vendor
preset: enabled)
Active: active (running) since Tue 2021-08-17 06:22:23 UTC; 24min ago
```
Идем в redis-cli после установки:
```
# redis-cli
127.0.0.1:6379> INFO
# Server
redis_version:5.0.3
```
**Redis Cluster**

Cluster был добавлен в Redis v.2.4, и является сервисом мониторинга состояния мастер и слейв нод. Умеет отправлять уведомления о событиях, выполнять переключение между мастером и слейвом, если мастер вышел из строя.

Важные особенности cluster
- Для минимального запуска требуется 6 нод: 3 master и 3 slave;
- Несколько master-нод, у каждой может быть до 1000 слейвов;
- Поддерживает шардинг, репликацию, переключение мастера и синхронизацию данных.

**Redis Sentinel**

Sentinel был добавлен в Redis v.2.4, и является сервисом мониторинга состояния мастер и слейв нод.

Важные особенности sentinel
- Может работать как отдельный демон или как redis в режиме sentinel;
- В случае выхода из строя мастера, переключает его на один из слейвов;
- Для работы требуется минимум 3 ноды, чтобы собирать кворум на переключение мастера;
- Configuration provider. Клиенты подключаются к sentinel, чтобы узнать, кто сейчас мастер в кластере.

**Redis vs Memcached** - Memcached имеет смысл использовать только, когда нужна максимальная простота и у вас мало серверов, мало запросов и они точно не будут расти. Но даже в этом случае с задачей лучше справится redis.

# ELK

Стек ELK — это аббревиатура, используемая для описания стека, состоящего из трёх популярных проектов:
- Elasticsearch
- Logstash
- Kibana
Стек ELK предоставляет возможность:
- агрегировать журналы из всех ваших систем и приложений
- анализировать эти журналы
- создавать визуализации для мониторинга приложений и инфраструктуры, более быстрого устранения неполадок, анализа безопасности и многого другого
Есть также платный Elastic Cloud и коммерческая версия ELK-стека

**Beats** — это отправители данных с открытым исходным кодом, которые вы устанавливаете в качестве агентов на своих серверах для отправки данных в Elasticsearch. На данный момент есть Auditbeat, Filebeat, Functionbeat, Heartbeat, Journalbeat, Metrics, Packetbeat, Winlogbeat. Мы посмотрим только на Filebeat

**Elasticsearch** — это распределённая, поисковая и аналитическая система, которая является сердцем ELK-стека. Он централизованно хранит данные для поиска, точной настройки релевантности и мощной аналитики, легко масштабируется. Все данные, которые будут писаться системой поставки, будут оседать и индексироваться в Elasticsearch

На основе Elasticsearch строят не только системы поставки логов, но и сервисы для поиска бизнесовых данных для пользователей, например, ebay classifieds. Данные в виде документов поставляются через API или тулзы вроде Logstash или Beats. После записи в базу поверх данных автоматически строятся индексы для быстрого поиска по полям через API или Kibana

**Установим Elasticsearch на Debian 10:**
```
# apt update && apt install gnupg apt-transport-https <--зависимости
# wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key
add - <--добавляем gpg-ключ
# echo "deb [trusted=yes] https://mirror.yandex.ru/mirrors/elastic/7/ stable
main" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list <--добавляем
репозиторий в apt
# apt update && apt-get install elasticsearch <--устанавливаем elastic
# systemctl daemon-reload <--обновляем конфиги systemd
# systemctl enable elasticsearch.service <--включаем юнит
# systemctl start elasticsearch.service <--запускаем сервис
```
После установки базы первым делом обезопасьте её. И настройте бекапы

Проверяем, что сервер запустился:
```
# curl 'localhost:9200/_cluster/health?pretty'
{
"cluster_name" : "netology-logging",
"status" : "green",
"timed_out" : false,
"number_of_nodes" : 1,
"number_of_data_nodes" : 1,
"active_primary_shards" : 1,
"active_shards" : 1,
"relocating_shards" : 0,
"initializing_shards" : 0,
"unassigned_shards" : 0,
"delayed_unassigned_shards" : 0,
"number_of_pending_tasks" : 0,
"number_of_in_flight_fetch" : 0,
"task_max_waiting_in_queue_millis" : 0,
"active_shards_percent_as_number" : 100.0
}
```
Немного настройки в /etc/elasticsearch/elasticsearch.yml:
```
cluster.name: netology-logging <--меняем имя кластера
node.name: node-1 <--меняем название ноды, если нужно
node.roles: [ master, data, ingest ] <--какую функцию будет выполнять эта нода
cluster.initial_master_nodes: ["node-1"] <--узлы, участвующие в голосовании по
выбору мастера
discovery.seed_hosts: ["ip-адрес"] <--список возможных мастеров кластера
path.data: /var/lib/elasticsearch <-где храним данные
path.logs: /var/log/elasticsearch <--куда пишем логи
network.host: 0.0.0.0 <--какой ip слушает хост
# systemctl restart elasticsearch
```
**Kibana** — это бесплатный и открытый пользовательский интерфейс, который позволяет визуализировать данные Elasticsearch. Интерфейс отчасти похож на Grafana

Возможности Kibana:
- визуализация данных
- аналитика
- мониторинг и алертинг
- ML

**Установим Kibana на Debian 10:**
```
# apt install kibana <--установка
# systemctl daemon-reload <--обновляем конфиги systemd
# systemctl enable kibana.service <--включаем юнит
# systemctl start kibana.service <--запускаем сервис
```
Настройки в /etc/kibana/kibana.yml:
```
server.host: "0.0.0.0" <--открываем интерфейс в мир
# systemctl restart kibana
```
**Logstash** — это сервис сбора данных с открытым исходным кодом и с возможностями конвейерной обработки в реальном времени. Logstash может динамически объединять данные из разрозненных источников и нормализовывать данные в места назначения по вашему выбору. Брать любые данные, парсить, нормализовывать их и писать в Elasticsearch или в любой поддерживаемый провайдер. Хранит стейт в файле

Конфигурация Logstash делится на:
- inputs. Отвечает за то, откуда Logstash возьмёт данные, например, из файла, syslog, stdin или redis
- filters. Как logstash изменит данные, которые пришли из inputs. Какие поля удалит, какие поменяет
- outputs. Куда после преобразования данные будут отправлены: в elasticsearch или file, например
- codecs. Сериализация. Например, преобразование строки в json или наоборот
**Установим Logstash на Debian 10:**
```
# apt install logstash <--установка
# systemctl daemon-reload <--обновляем конфиги systemd
# systemctl enable logstash.service <--включаем юнит
# systemctl start logstash.service <--запускаем сервис
```
Настроим поставку access-лога nginx в elasticsearch:
```
input {
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
  }
}
filter {
  grok {
    match => { "message" => "%{IPORHOST:remote_ip} - %{DATA:user_name} \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{DATA:url} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\"" }
  }
  mutate {
    remove_field => [ "host" ]
  }
}
  output {
    elasticsearch {
      hosts => "178.154.215.248"
      data_stream => "true"
    }
}
```
**Filebeat** — это легковесный агент для пересылки и централизации данных из файлов. Устанавливается как демон на сервера. Основное отличие от Logstash — лёгкость и скорость, но с урезанным функционалом пайплайнов. Так же, как Logstash, хранит стейт в файле и может менять данные перед отправкой

Конфигурация состоит из двух компонентов:
- inputs. Как и откуда будут читаться данные для поставки
- processors. Позволяет незначительно менять данные в пайплайне
- harvester (комбайн). Запускается на каждый файл, который читает Filebeat, собирательное название каждой поставки данных

**Установим Filebeat на Debian 10:**
```
# apt install filebeat <--установка
# systemctl daemon-reload <--обновляем конфиги systemd
# systemctl enable filebeat.service <--включаем юнит
# systemctl start filebeat.service <--запускаем сервис
```
конфиг для отправки в Logstash:
```
# меняем конфиг Logstash
input {
  beats {
    port => 5044
  }
}
# меняем конфиг Filebeat
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/nginx/access.log
processors:
  - drop_fields: <--удаляются системные поля, которые добавил filebeat
    fields: ["beat", "input_type", "prospector", "input", "host", "agent", "ecs"]

output.logstash:
  hosts: ["178.154.215.248:5044"]
```

# Очереди RabbitMQ

**Очереди сообщений** предоставляют асинхронный протокол передачи данных. Это означает, что отправитель и получатель сообщения не обязаны взаимодействовать с очередью сообщений одновременно. Размещённые в очереди сообщения хранятся до тех пор, пока получатель не получит их.

**Особенности очередей сообщений**
- Очереди сообщений имеют неявные или явные ограничения на размер данных, которые могут передаваться в одном сообщении, и количество сообщений, которые могут оставаться в очереди.
- Многие реализации очередей сообщений функционируют внутренне: внутри операционной системы или внутри приложения. Такие очереди существуют только для целей этой системы.
- Другие реализации позволяют передавать сообщения между различными компьютерными системами, потенциально подключая несколько приложений и несколько операционных систем. Эти системы очередей сообщений обычно обеспечивают расширенную функциональность для обеспечения устойчивости, чтобы гарантировать, что сообщения не будут «потеряны» в случае сбоя системы.

**FIFO и LIFO (ФИФО и ЛИФО)**

Сервисы обмена сообщениями между серверами делятся на два типа:
- Очереди (ФИФО-метод) — «первый пришел и первый ушел» (First in First out). Принцип FIFO означает: сообщение, которое первым попало в очередь, первым же отправляется на обработку.
- Стеки (LIFO) — «пришел последним, а ушел первым» (Last in First out). В отличие от системы FIFO, стек можно представить в виде стопки книг: вы кладете книги друг на друга и сначала берете верхние книги

**Брокеры сообщений**

Для реализации очередей сообщений используются брокеры сообщений:
- RabbitMQ — самый популярный брокер, держит большую нагрузку, умеет кластеризоваться;
- Kafka — больше напоминает распределенный журнал, умеет разбивать топики на шарды, используется в хайлоаде, так же умеет кластеризоваться, для работы нужен Zookeeper.
- ActiveMQ — брокер сообщений, написанный на Java. Компонент, реализующий очереди называется Artemis, реализует спецификацию JMS 1.1.

**RabbitMQ** — программный брокер сообщений на основе стандарта.

**AMQP** — тиражируемое связующее программное обеспечение, ориентированное на обработку сообщений. Создан на основе системы Open Telecom Platform, написан на языке Erlang, в качестве движка базы данных для хранения сообщений использует Mnesia.
- Умеет масштабироваться горизонтально (ноды можно объединять в кластер) и вертикально, имеет большой набор плагинов (например management, shovel, etc...);
- Имеет огромное количество поддерживаемых клиентов: Ruby, Python, Go, Java...

**Архитектура очередей**

**Протокол AMQP**
- Сообщение (message) — единица передаваемых данных, основная его часть (содержание) никак не интерпретируется сервером, к сообщению могут быть присоединены структурированные заголовки.
- Точка обмена (exchange) — в неё отправляются сообщения. Точка обмена распределяет сообщения в одну или несколько очередей. При этом в точке обмена сообщения не хранятся.
- Точки обмена бывают трёх типов:
- - fanout — сообщение передаётся во все прицепленные к ней очереди;
- - direct — сообщение передаётся в очередь с именем, совпадающим с ключом маршрутизации (routing key) (ключ маршрутизации указывается при отправке сообщения);
- - topic — нечто среднее между fanout и direct, сообщение передаётся в очереди, для которых совпадает маска на ключ маршрутизации, например, app.notification.sms.# — в очередь будут доставлены все сообщения, отправленные с ключами, начинающимися с app.notification.sms.
- Очередь (queue) — здесь хранятся сообщения до тех пор, пока не будут забраны клиентом. Клиент всегда забирает сообщения из одной или нескольких очередей

**Терминология**
- Exchange — сущность которая получает сообщения от приложений и при необходимости перенаправляет их в очереди сообщений.
- Binding — отношение между очередью сообщений и точками обмена.
- Routing key — виртуальный адрес, который точка обмена использует для принятия решения о дальнейшей маршрутизации.

**Exchange** - Принимает сообщения от поставщика и направляет их в message queue в соответствии с предопределёнными критериями. Такие критерии называют bindings. Exchange — механизм согласования и маршрутизации сообщений. На основе сообщений и их параметров (bindings) принимают решение о перенаправлении в очередь или другой exchange. Не хранят сообщения. Термин exchange означает алгоритм и экземпляр алгоритма. Также говорят exchange type и exchange instance. AMQP определяет набор стандартных типов exchange. Приложения могут создавать свои exchange instance.

**Алгоритмы маршрутизации**
Существует несколько стандартных типов exchange, описанных в стандарте. Из них два являются важными:
- Direct exchange — маршрутизация на основе routing key. Базовый exchange — это direct exchange
- Topic exchange — маршрутизация на основе шаблона маршрутизации.
У RabbitMQ существует еще Funout отправляет сообщения во все связанные очереди без проверки ключа маршрутизации или заголовка сообщения. И Headers работает по заголовкам, которые удобней и легче ключей маршрутизации.

**Routing Key**
В общем случае exchange:
- проверяет свойства сообщения, поля заголовка и содержимое его тела;
- используя эти и, возможно, данные из других источников, решает, как направить сообщение.
В большинстве простых случаев exchange рассматривает одно ключевое поле, которое мы называем Routing Key. Routing Key — это виртуальный адрес, который сервер exchange может использовать для принятия решения о направлении сообщения

- Для маршрутизации типа point-to-point ключом маршрутизации обычно является имя очереди сообщений.
- Для маршрутизации pub-sub ключ маршрутизации обычно является значением иерархии топика.
- В более сложных случаях ключ маршрутизации может быть объединен с маршрутизацией по полям заголовка сообщения и/или его содержанием.

**Message Queue**
Когда клиентское приложение создает очередь сообщений, оно может указать следующие свойства:
- name — если не указано, сервер сам выбирает имя и отправляет его клиенту. Как правило, когда приложения совместно используют очередь сообщений, они заранее договариваются об имени очереди сообщений, и когда приложение нуждается в очереди сообщений для своих собственных целей, оно позволяет серверу предоставлять имя.
- exclusive — если этот параметр установлен, то очередь существует, пока существует текущее соединение. Очередь удаляется при разрыве подключения.
- durable — если установлен, очередь существует и активна при перезагрузке сервера. Очередь может потерять сообщения посланные во время перезагрузки сервера
**Жизненный цикл сообщения**
- Сообщение создается producer.
- Producer маркирует сообщение с помощью маршрутной информации.
- Затем producer отправляет сообщение в exchange.
- Exchange (обычно) направляет его в набор очередей.
- Когда сообщение поступает в очередь сообщений, она немедленно пытается передать его потребителю через AMQP.
- Если отправить не удается возвращаем Producer или dead-letter.
- После получения ставим ACK и сообщение из хранилища удаляется.

**Producer** — клиентское приложение, которое публикует сообщения в exchange. По аналогии с устройством электронной почты, можно заметить, что producer не отправляет сообщения непосредственно в очередь (message queue). Иное поведение нарушило бы модель AMQ. Это было бы похоже на жизненный цикл сообщения электронной почты: разрешение электронной почты, обход таблиц маршрутизации MTA и попадание непосредственно в почтовый ящик. Это сделало бы невозможной вставку промежуточной фильтрации и обработки, например, обнаружение спама. Модель AMQ использует тот же принцип, что и система электронной почты: все сообщения отправляются в одну точку exchange или MTA, который проверяет сообщения на основе правил и информации, которая скрыта от отправителя, и направляет их к точкам распространения, которые также скрыты от отправителя.

**Consumer** — клиентское приложение, которое получает сообщения из очереди сообщений. Наша аналогия с электронной почтой начинает разрушаться, когда мы смотрим на consumer (получателей). Почтовые клиенты пассивны — они могут читать почтовые ящики, но они не оказывают никакого влияния на то, как эти почтовые ящики заполняются. С помощью AMQP consumer также может быть пассивным, как и почтовые клиенты. То есть мы можем написать приложение, которое прослушивает определённую очередь сообщений и просто обрабатывает поступающую информацию. При этом очередь сообщений должна быть готова до старта приложения и должна быть «привязана» к нему.

Возможности Consumer
- создавать/удалять очереди сообщений;
- определять способ заполнений очереди используя bindings;
- выбирать разные exchanges, что может полностью изменить семантику маршрутизации.

Работа с брокером из командной строки

Для просмотра доступных очередей:
```
rabbitmqctl list_queues
```
Для просмотра содержимого очереди:
```
rabbitmqadmin get queue='hello'
```
**Простейший Producer**

Код написан на языке Python, для работы нужно установить библиотеку pika командой pip install pika:
```
#!/usr/bin/env python
# coding=utf-8
import pika
connection =
pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()
channel.queue_declare(queue='hello')
channel.basic_publish(exchange='', routing_key='hello', body='Hello Netology!')
connection.close()
```
**Простейший Consumer**
```
#!/usr/bin/env python
# coding=utf-8
import pika
connection =
pika.BlockingConnection
(pika.ConnectionParameters('localhost'))
channel = connection.channel()
channel.queue_declare(queue='hello')
def callback
(ch, method, properties, body):
print(" [x] Received %r" % body)
channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=False)
channel.start_consuming
()
```
**Создаем кластер из двух нод**
- Rabbitmq-ноды и CLI-инструменты (например, rabbitmqctl) используют Erlang cookie-файл для аутентификации между собой.
- Каждый нода кластера должна иметь такой файл с одинаковым содержимым на всех нодах кластера.
- Содержимое файла /var/lib/rabbitmq/.erlang.cookie на app02 должно совпадать с содержимым файла на app01.
- Права на файл 400, владелец/группа – rabbitmq:rabbitmq.

Создание политики репликации - Для того чтобы реплицировать очереди, необходимо создать политику, которая будет описывать режим и тип репликации. Политики могут создаваться в любое время. Политики могут применяться ко всем очередям или к выборочным очередям (с фильтрацией имени очереди по шаблону регулярного выражения) Можно создавать не реплицируемые очереди, а потом позже делать их реплицируемыми через создание политики.
```
rabbitmqctl set_policy ha-all ""
'{"ha-mode":"all","ha-sync-mode":"automatic"}'
```
Объединение нод в кластер - Для того, чтобы создать кластер из двух нод мы присоединим, например, вторую ноду к первой ноде(app01).

Для этого на второй ноде:
```
rabbitmqctl stop_app
rabbitmqctl join_cluster {ip_addr or dns name}
rabbitmqctl start_app
rabbitmqctl cluster_status
```
**Полезные CLI команды**

Просмотр списка очередей:
```
rabbitmqctl list_queues
```
Просмотр списка очередей с выводом имен политик,которые применены к этим очередям:
```
rabbitmqctl list_queues name policy pid slave_pid
```
Ручная синхронизация очереди:
```
rabbitmqctl sync_queue <имя очереди>
```
Отмена синхронизации очереди:
```
rabbitmqctl cancel_sync_queue <имя очереди>
```
Проверка состояния RabbitMQ ноды:
```
rabbitmqctl node_health_check
```
Просмотр статуса RabbitMQ-ноды:
```
rabbitmqctl status
```
Полный отчет (включая состояние кластера, нод, политик, параметров, пользователей, вирт.хостов и т.д.):\
```
rabbitmqctl report | less
```
Больше команд доступно по:
```
rabbitmqctl --help
```
